{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11483687,"sourceType":"datasetVersion","datasetId":7197473},{"sourceId":11582849,"sourceType":"datasetVersion","datasetId":7262401}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cd /","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T04:01:33.286002Z","iopub.execute_input":"2025-04-27T04:01:33.286221Z","iopub.status.idle":"2025-04-27T04:01:33.293825Z","shell.execute_reply.started":"2025-04-27T04:01:33.286197Z","shell.execute_reply":"2025-04-27T04:01:33.293027Z"}},"outputs":[{"name":"stdout","text":"/\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q llama-index-llms-langchain llama-index-graph-stores-nebula ipython-ngql nebula3-python llama-index-retrievers-bm25 llama-index llama-index-vector-stores-qdrant llama-index-embeddings-huggingface llama-index-embeddings-fastembed -U FlagEmbedding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T04:01:33.295221Z","iopub.execute_input":"2025-04-27T04:01:33.295489Z","iopub.status.idle":"2025-04-27T04:03:08.767992Z","shell.execute_reply.started":"2025-04-27T04:01:33.295466Z","shell.execute_reply":"2025-04-27T04:03:08.767020Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.3/331.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m669.3/669.3 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.7/327.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.0/859.0 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.2/119.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.6/263.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.2/661.2 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for FlagEmbedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for warc3-wet-clueweb09 (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for cbor (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# key \nGEMINI_API_KEY=\"AIzaSyD-EmTTKBJjdbQgwAzUgx03hVn8aklZfUA\"\n\nQDRANT_URL=\"https://6afe4efa-992e-4445-af88-4f6a7ce7a4b1.europe-west3-0.gcp.cloud.qdrant.io\"\nQDRANT_API_KEY=\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIiwiZXhwIjoxNzQ3ODQ4MTA4fQ.RFFGb0pqV1GnZ_H1HymCJ6kINq7e0B9AfFia2qyE6hc\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T04:07:39.702714Z","iopub.execute_input":"2025-04-27T04:07:39.703211Z","iopub.status.idle":"2025-04-27T04:07:39.707174Z","shell.execute_reply.started":"2025-04-27T04:07:39.703182Z","shell.execute_reply":"2025-04-27T04:07:39.706629Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# configuration\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nfrom llama_index.core import Settings, Document\n\nCOLLECTION_NAME_PARENT = \"vietnamese_legal_parent\"\nCOLLECTION_NAME_CHILD = \"vietnamese_legal_child\"\nNUM_DOCUMENTS = 30  # Number of documents to process\nPARENT_CHUNK_SIZE = 1500\nCHILD_CHUNK_SIZE = 300\nPARENT_CHUNK_OVERLAP = 200\nCHILD_CHUNK_OVERLAP = 50\nMIN_LENGTH = 300\nMAX_LENGTH = 1500\n\nSettings.embed_model = HuggingFaceEmbedding(\n    model_name=\"BAAI/bge-m3\",\n    device=\"cuda\",\n    embed_batch_size=64\n)\n\nclient = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T04:16:31.397399Z","iopub.execute_input":"2025-04-27T04:16:31.397999Z","iopub.status.idle":"2025-04-27T04:16:50.989943Z","shell.execute_reply.started":"2025-04-27T04:16:31.397978Z","shell.execute_reply":"2025-04-27T04:16:50.989081Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50de94f7f5024633bbc131ec2fca0ccb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28be4c45295b4176a56f79f31be4ee62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/15.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d08a2c77d8e4d70b11bd7a2573fc204"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6d4a5cdeb8c48c99a263d75f920ba64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cae7139d11214c3c83de686b83894308"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"604b23b3d3ac4fc58f9f898a4599d305"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc15c85b2b0c4093860dabf527eaf02b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13d7be60adde46268421bbff78212784"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"680b1b7f88b74664b328dfcb36da463a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93aa41332a6a4f58b3bcab9887fc2f64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54f5312badf04a87a9ecb616a224740b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d52395dd42194493ab070dbca84b1c0d"}},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# import libraries\nimport json\nimport os\nimport pickle\nimport logging\nfrom tqdm import tqdm\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http import models\nfrom dotenv import load_dotenv\nfrom typing import List, Dict\n\nfrom llama_index.core import Document, StorageContext\nfrom qdrant_client import models\nfrom llama_index.core.node_parser import HierarchicalNodeParser\nfrom llama_index.core.node_parser.relational.hierarchical import get_root_nodes, get_child_nodes\nfrom llama_index.vector_stores.qdrant import QdrantVectorStore\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T04:07:46.958974Z","iopub.execute_input":"2025-04-27T04:07:46.959657Z","iopub.status.idle":"2025-04-27T04:08:15.218979Z","shell.execute_reply.started":"2025-04-27T04:07:46.959630Z","shell.execute_reply":"2025-04-27T04:08:15.218443Z"}},"outputs":[{"name":"stderr","text":"2025-04-27 04:08:01.853668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745726882.036094      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745726882.090052      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# normalize\ndef normalize(document_text): # Giữ tên tham số là document_text\n    # Chuẩn hóa dữ liệu đầu vào - THAY THẾ TẤT CẢ XUỐNG DÒNG BẰNG DẤU CÁCH\n    normalized_text = document_text.replace('\\r\\n', '\\n').replace('\\r', '\\n').replace('\\n', ' ')\n    \n    # Tìm vị trí đầu tiên của Chương hoặc Điều\n    chapter_match = re.search(r'Chương\\s+[IVXLCDM]+', normalized_text)\n    article_match = re.search(r'Điều\\s+\\d+\\.', normalized_text)\n    \n    # Xác định vị trí bắt đầu của văn bản cần giữ lại\n    start_pos = 0\n    if chapter_match and article_match:\n        # Lấy vị trí xuất hiện đầu tiên giữa Chương và Điều\n        start_pos = min(chapter_match.start(), article_match.start())\n    elif chapter_match:\n        start_pos = chapter_match.start()\n    elif article_match:\n        start_pos = article_match.start()\n    \n    # Cắt bỏ mọi nội dung trước đoạn đầu tiên\n    normalized_text = normalized_text[start_pos:]\n    \n    # Áp dụng các pattern chuẩn để thêm xuống dòng cho các mẫu cần thiết\n    patterns = [\n        # Đảm bảo Chương bắt đầu dòng mới\n        (r\"(?<=\\S)\\s+(Chương\\s+[IVXLCDM]+)\", r\"\\n\\1\"),\n        # Đảm bảo bắt đầu của văn bản nếu là Chương\n        (r\"^(Chương\\s+[IVXLCDM]+)\", r\"\\1\"),\n        # Đảm bảo Điều bắt đầu dòng mới\n        (r\"(?<=\\S)\\s+(Điều\\s+\\d+\\.)\", r\"\\n\\1\"),\n        # Đảm bảo bắt đầu của văn bản nếu là Điều\n        (r\"^(Điều\\s+\\d+\\.)\", r\"\\1\"),\n    ]\n\n    for pattern, repl in patterns:\n        normalized_text = re.sub(pattern, repl, normalized_text)\n    \n    # Dọn dẹp cuối cùng: Xóa khoảng trắng thừa ở đầu/cuối mỗi dòng và loại bỏ dòng trống\n    lines = normalized_text.split('\\n')\n    cleaned_lines = [line.strip() for line in lines if line.strip()]\n    final_text = '\\n'.join(cleaned_lines)\n    \n    return final_text","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-04-27T04:08:29.809528Z","iopub.execute_input":"2025-04-27T04:08:29.810091Z","iopub.status.idle":"2025-04-27T04:08:29.816119Z","shell.execute_reply.started":"2025-04-27T04:08:29.810065Z","shell.execute_reply":"2025-04-27T04:08:29.815301Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# parse_text_to_structure\nimport re\nimport json\n\ndef _parse_dieu_khoan_diem(text_segment):\n    \"\"\"Helper to parse Điều and their Khoản/Điểm.\"\"\"\n    dieu_dict = {}\n    # Split by Điều\n    dieu_splits = re.split(r'(?=Điều\\s+\\d+\\.?)', text_segment) # re.split() finds every position in text_segment where “Điều <number>” begins, and splits just before it!\n    dieu_segments = []\n    \n    # Skip potential introductory content before first Điều\n    for seg in dieu_splits:\n        if seg.strip() and seg.strip().startswith('Điều'):\n            dieu_segments.append(seg.strip())\n\n    for dieu_segment in dieu_segments:\n        dieu_match = re.match(r'(Điều\\s+\\d+\\.?)\\s*(.*?)(?:\\n|$)', dieu_segment, re.DOTALL)\n        if not dieu_match:\n            continue\n\n        dieu_num = dieu_match.group(1).strip().rstrip('.')\n        dieu_desc = dieu_match.group(2).strip()\n        dieu_content_text = dieu_segment[len(dieu_match.group(0)):].strip()\n\n        dieu_dict[dieu_num] = {\n            'content': dieu_desc + '\\n' + dieu_content_text if dieu_content_text else dieu_desc, # is this necessary?\n            'length': len(dieu_desc) + len(dieu_content_text) if dieu_content_text else len(dieu_desc)\n        }\n\n    return dieu_dict\n\ndef merge_short_segments(segments, min_length=400, max_length=2000):\n    \"\"\"\n    Merge segments that are too short.\n    Returns merged segments with parent links showing which originals were merged.\n    \"\"\"\n    if not segments:\n        return []\n    \n    # Sort by Điều number\n    sorted_segments = sorted(segments, key=lambda x: int(re.search(r'\\d+', x[0]).group()))\n    \n    merged_segments = []\n    current_segment = {\n        'keys': [sorted_segments[0][0]], \n        'content': sorted_segments[0][1]['content'],\n        'length': sorted_segments[0][1]['length']\n    }\n    \n    for i in range(1, len(sorted_segments)):\n        key, content_obj = sorted_segments[i]\n        \n        # If current segment is too short or adding next segment won't exceed max_length\n        if (current_segment['length'] < min_length or \n            (current_segment['length'] + content_obj['length'] < max_length)):\n            # Merge this segment\n            current_segment['keys'].append(key)\n            current_segment['content'] += '\\n\\n' + content_obj['content']\n            current_segment['length'] += content_obj['length'] + 2  # +2 for newlines\n        else:\n            # Current segment is long enough, store it and start a new one\n            merged_segments.append(current_segment)\n            current_segment = {\n                'keys': [key],\n                'content': content_obj['content'],\n                'length': content_obj['length']\n            }\n    \n    # Don't forget to add the last segment\n    merged_segments.append(current_segment)\n    \n    # Convert merged segments to final format\n    result = {}\n    \n    for segment in merged_segments:\n        if len(segment['keys']) == 1:\n            # Single segment, use original key\n            key = segment['keys'][0]\n            result[key] = {\n                'content': segment['content']\n            }\n        else:\n            # Merged segment, create new key and add parent_links\n            merged_key = ' + '.join(segment['keys'])\n            result[merged_key] = {\n                'content': segment['content'],\n                'parent_links': segment['keys']  # These are the original Điều numbers\n            }\n    \n    return result\n\ndef parse_text_to_structure(text, min_length=400, max_length=2000):\n    \"\"\"\n    Parses Vietnamese legal text with merging of short segments.\n    \n    Output format:\n    {\n        'Chương I': {\n            'Điều 1': {'content': 'Content of Article 1'},\n            'Điều 2+Điều 3': {\n                'content': 'Combined content...',\n                'parent_links': ['Điều 2', 'Điều 3']\n            },\n            ...\n        },\n        ... OR if no chapters exist:\n        'Điều 1': {'content': 'Content of Article 1'},\n        'Điều 2 + Điều 3': {\n            'content': 'Combined content...',\n            'parent_links': ['Điều 2', 'Điều 3']\n        },\n        ...\n    }\n    \"\"\"\n    data = {}\n    text = text.strip()\n    \n    # Check if there are any chapters\n    has_chapters = bool(re.search(r'Chương\\s+[IVXLCDM]+', text))\n    \n    if has_chapters:\n        # Split by chapters\n        chapter_splits = re.split(r'(?=Chương\\s+[IVXLCDM]+)', text)\n        \n        # Skip non-chapter introductory content\n        chapter_segments = []\n        for seg in chapter_splits:\n            if seg.strip() and seg.strip().startswith('Chương'):\n                chapter_segments.append(seg.strip())\n        \n        for chapter in chapter_segments:\n            chapter_match = re.match(r'^(Chương\\s+[IVXLCDM]+)\\s+(.*?)(?:\\n|$)', chapter, re.DOTALL)\n            if not chapter_match:\n                continue\n                \n            chapter_num = chapter_match.group(1)   # e.g., \"Chương I\" (index 1 is not counted as usual)!\n            chapter_title = chapter_match.group(2).strip()\n            \n            # Create chapter entry with title\n            data[chapter_num] = {'title': chapter_title}\n            \n            # Extract remaining content after chapter title\n            content_after_title = chapter[len(chapter_match.group(0)):].strip()\n            \n            # Parse Điều within the chapter\n            dieu_dict = _parse_dieu_khoan_diem(content_after_title)\n            \n            # Get segments as a list of (key, value) tuples\n            dieu_segments = list(dieu_dict.items())\n            \n            # Merge short segments\n            merged_dict = merge_short_segments(dieu_segments, min_length, max_length)\n            \n            # Add merged segments to this chapter\n            data[chapter_num].update(merged_dict)\n    else:\n        # No chapters, parse Điều directly\n        dieu_dict = _parse_dieu_khoan_diem(text)\n        \n        # Get segments as a list of (key, value) tuples. This is necessary for merging!\n        dieu_segments = list(dieu_dict.items())\n        \n        # Merge short segments\n        merged_dict = merge_short_segments(dieu_segments, min_length, max_length)\n        \n        # Add merged segments to root data\n        data.update(merged_dict)\n            \n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T04:08:36.903624Z","iopub.execute_input":"2025-04-27T04:08:36.903921Z","iopub.status.idle":"2025-04-27T04:08:36.917659Z","shell.execute_reply.started":"2025-04-27T04:08:36.903892Z","shell.execute_reply":"2025-04-27T04:08:36.917040Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# prepare_vietnamese_legal_documents\ndef prepare_vietnamese_legal_documents(\n    legal_texts: List[str], \n    document_ids: List[str],\n    min_length: int = MIN_LENGTH,\n    max_length: int = MAX_LENGTH\n) -> List[Document]:\n    \"\"\"Prepare LlamaIndex Document objects from Vietnamese legal texts.\"\"\"\n    print(f\"Preparing {len(legal_texts)} Vietnamese legal documents\")\n    documents = []\n    \n    for i, text in enumerate(tqdm(legal_texts, desc=\"Processing documents\")):\n        doc_id = document_ids[i]\n        \n        # Normalize the text\n        normalized_text = normalize(text)\n        \n        # Parse the structure\n        structured_data = parse_text_to_structure(normalized_text, min_length, max_length)\n        \n        # Process each section (could be chapter or direct Điều)\n        for key, content in structured_data.items():\n            if key.startswith(\"Chương\"):\n                # Process chapter\n                chapter_title = content.get('title', '')\n                \n                for article_key, article_content in content.items():\n                    if article_key == 'title':\n                        continue\n                        \n                    if isinstance(article_content, dict) and 'content' in article_content:\n                        doc_text = article_content['content']\n                        metadata = {\n                            'document_id': doc_id,\n                            'chapter': key,\n                            'chapter_title': chapter_title,\n                            'article': article_key\n                        }\n                        \n                        if 'parent_links' in article_content:\n                            metadata['parent_links'] = article_content['parent_links'][:2]\n                            metadata['is_merged'] = True\n                        \n                        documents.append(Document(text=doc_text, metadata=metadata))\n            \n            elif key.startswith(\"Điều\") or \"+\" in key:\n                # Process direct article or merged articles\n                if isinstance(content, dict) and 'content' in content:\n                    doc_text = content['content']\n                    metadata = {\n                        'document_id': doc_id,\n                        'article': key\n                    }\n                    \n                    if 'parent_links' in content:\n                        metadata['parent_links'] = content['parent_links'][:2]\n                        metadata['is_merged'] = True\n                    \n                    documents.append(Document(text=doc_text, metadata=metadata))\n    \n    print(f\"Created {len(documents)} document objects\")\n    return documents","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T04:38:23.518430Z","iopub.execute_input":"2025-04-27T04:38:23.518699Z","iopub.status.idle":"2025-04-27T04:38:23.526788Z","shell.execute_reply.started":"2025-04-27T04:38:23.518683Z","shell.execute_reply":"2025-04-27T04:38:23.526022Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# prepare documents part 2\n# prepare_vietnamese_legal_documents\ndef prepare_vietnamese_legal_documents(\n    legal_texts: List[str], \n    document_ids: List[str],\n    min_length: int = MIN_LENGTH,\n    max_length: int = MAX_LENGTH\n) -> List[Document]:\n    \"\"\"Prepare LlamaIndex Document objects from Vietnamese legal texts.\"\"\"\n    print(f\"Preparing {len(legal_texts)} Vietnamese legal documents\")\n    documents = []\n    \n    # Create an empty list to store metadata separately\n    metadata_store = []\n    \n    for i, text in enumerate(tqdm(legal_texts, desc=\"Processing documents\")):\n        doc_id = document_ids[i]\n        \n        # Normalize the text\n        normalized_text = normalize(text)\n        \n        # Parse the structure\n        structured_data = parse_text_to_structure(normalized_text, min_length, max_length)\n        \n        # Process each section (could be chapter or direct Điều)\n        for key, content in structured_data.items():\n            if key.startswith(\"Chương\"):\n                # Process chapter\n                chapter_title = content.get('title', '')\n                \n                for article_key, article_content in content.items():\n                    if article_key == 'title':\n                        continue\n                        \n                    if isinstance(article_content, dict) and 'content' in article_content:\n                        doc_text = article_content['content']\n                        metadata = {\n                            'document_id': doc_id,\n                            'chapter': key,\n                            'chapter_title': chapter_title,\n                            'article': article_key\n                        }\n                        \n                        # Trim or reduce size of large metadata (parent_links and is_merged)\n                        if 'parent_links' in article_content:\n                            metadata['parent_links'] = article_content['parent_links'][:5]  # Only keep the first 5 links\n                            metadata['is_merged'] = True\n                        \n                        # Store metadata separately\n                        metadata_store.append(metadata)\n                        \n                        # Add text content for chunking, metadata will be linked later\n                        documents.append(Document(text=doc_text, metadata={}))\n            \n            elif key.startswith(\"Điều\") or \"+\" in key:\n                # Process direct article or merged articles\n                if isinstance(content, dict) and 'content' in content:\n                    doc_text = content['content']\n                    metadata = {\n                        'document_id': doc_id,\n                        'article': key\n                    }\n                    \n                    # Trim or reduce size of large metadata (parent_links and is_merged)\n                    if 'parent_links' in content:\n                        metadata['parent_links'] = content['parent_links'][:5]  # Only keep the first 5 links\n                        metadata['is_merged'] = True\n                    \n                    # Store metadata separately\n                    metadata_store.append(metadata)\n                    \n                    # Add text content for chunking, metadata will be linked later\n                    documents.append(Document(text=doc_text, metadata={}))\n    \n    print(f\"Created {len(documents)} document objects\")\n    return documents, metadata_store\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T04:47:41.369198Z","iopub.execute_input":"2025-04-27T04:47:41.369791Z","iopub.status.idle":"2025-04-27T04:47:41.378613Z","shell.execute_reply.started":"2025-04-27T04:47:41.369768Z","shell.execute_reply":"2025-04-27T04:47:41.378034Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"def create_qdrant_collections():\n    \"\"\"Create Qdrant collections for parent and child nodes\"\"\"\n    # Get Qdrant credentials from env\n    qdrant_url = QDRANT_URL\n    qdrant_api_key = QDRANT_API_KEY\n    \n    if not qdrant_url or not qdrant_api_key:\n        raise ValueError(\"QDRANT_URL and QDRANT_API_KEY must be set in the environment\")\n    \n    # Initialize Qdrant client\n    client = QdrantClient(url=qdrant_url, api_key=qdrant_api_key)\n    print(f\"Connected to Qdrant at {qdrant_url}\")\n    \n    # Create collections for parent and child nodes if they don't exist\n    try:\n        # Parent collection\n        client.get_collection(COLLECTION_NAME_PARENT)\n        print(f\"Collection {COLLECTION_NAME_PARENT} already exists\")\n    except Exception:\n        client.create_collection(\n            collection_name=COLLECTION_NAME_PARENT,\n            vectors_config=models.VectorParams(\n                size=1024,  # bge-m3 embedding dimension\n                distance=models.Distance.COSINE\n            )\n        )\n        print(f\"Created collection {COLLECTION_NAME_PARENT}\")\n    \n    try:\n        # Child collection\n        client.get_collection(COLLECTION_NAME_CHILD)\n        print(f\"Collection {COLLECTION_NAME_CHILD} already exists\")\n    except Exception:\n        client.create_collection(\n            collection_name=COLLECTION_NAME_CHILD,\n            vectors_config=models.VectorParams(\n                size=1024,  # bge-m3 embedding dimension\n                distance=models.Distance.COSINE\n            )\n        )\n        print(f\"Created collection {COLLECTION_NAME_CHILD}\")\n    \n    return client\n\ndef index_documents_to_qdrant(documents):\n    \"\"\"Index documents to Qdrant using hierarchical chunking\"\"\"\n    # Get Qdrant credentials from env\n    qdrant_url = QDRANT_URL\n    qdrant_api_key = QDRANT_API_KEY\n    \n    # Load embedding model\n    embed_model = Settings.embed_model\n    print(\"Loaded bge-m3 embedding model\")\n    \n    # Create hierarchical node parser\n    node_parser = HierarchicalNodeParser.from_defaults(\n        chunk_sizes=[PARENT_CHUNK_SIZE+400, CHILD_CHUNK_SIZE+400],\n        chunk_overlap=50\n    )\n    \n    # Parse all nodes hierarchically\n    print(\"Parsing nodes hierarchically...\")\n    all_nodes = node_parser.get_nodes_from_documents(documents)\n    \n    # Separate parent and child nodes\n    # parent_nodes = [n for n in all_nodes if n.metadata.get('level') == 0]\n    # child_nodes = [n for n in all_nodes if n.metadata.get('level') == 1]\n    parent_nodes = get_root_nodes(all_nodes)\n    child_nodes = get_child_nodes(parent_nodes, all_nodes)\n    \n    print(f\"Created {len(parent_nodes)} parent nodes and {len(child_nodes)} child nodes\")\n    \n    # Create Qdrant vector stores\n    parent_vector_store = QdrantVectorStore(\n        client=QdrantClient(url=qdrant_url, api_key=qdrant_api_key),\n        collection_name=COLLECTION_NAME_PARENT\n    )\n    \n    child_vector_store = QdrantVectorStore(\n        client=QdrantClient(url=qdrant_url, api_key=qdrant_api_key),\n        collection_name=COLLECTION_NAME_CHILD\n    )\n    \n    # Create storage contexts\n    parent_storage_context = StorageContext.from_defaults(vector_store=parent_vector_store)\n    child_storage_context = StorageContext.from_defaults(vector_store=child_vector_store)\n    \n    # Add nodes to docstore\n    parent_storage_context.docstore.add_documents(parent_nodes)\n    child_storage_context.docstore.add_documents(child_nodes)\n    \n    # Add documents to Qdrant with embeddings\n    print(\"Adding parent nodes to Qdrant...\")\n    for node in tqdm(parent_nodes, desc=\"Embedding parent nodes\"):\n        embedding = embed_model.get_text_embedding(node.get_content())\n\n        point = models.PointStruct(\n            id=node.id_, vector= embedding, payload={\"content\": node.get_content()}\n        )\n        \n        client.upload_points(\n        collection_name=COLLECTION_NAME_PARENT,\n        points=[point]\n        )\n    \n    print(\"Adding child nodes to Qdrant...\")\n    for node in tqdm(child_nodes, desc=\"Embedding child nodes\"):\n        embedding = embed_model.get_text_embedding(node.get_content())\n        point = models.PointStruct(\n            id=node.id_, vector= embedding, payload={\"content\": node.get_content()}\n        )\n        \n        client.upload_points(\n            collection_name=COLLECTION_NAME_CHILD,\n            points=[point]\n        )\n    \n    # Save node IDs and metadata to pickle for later reference\n    node_info = {\n        'parent_node_ids': [node.node_id for node in parent_nodes],\n        'child_node_ids': [node.node_id for node in child_nodes],\n        'document_ids': list(set([doc.metadata.get('document_id') for doc in documents]))\n    }\n    \n    with open(\"/kaggle/working/qdrant_node_info.pkl\", \"wb\") as f:\n        pickle.dump(node_info, f)\n    \n    print(\"Saved node information to qdrant_node_info.pkl\")\n    print(\"Indexing complete!\")\n    \n    return node_info, all_nodes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T04:28:22.029487Z","iopub.execute_input":"2025-04-27T04:28:22.029823Z","iopub.status.idle":"2025-04-27T04:28:22.041421Z","shell.execute_reply.started":"2025-04-27T04:28:22.029800Z","shell.execute_reply":"2025-04-27T04:28:22.040652Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Setup logging + create qdrant collections\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\ncreate_qdrant_collections()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T04:09:43.296183Z","iopub.execute_input":"2025-04-27T04:09:43.296747Z","iopub.status.idle":"2025-04-27T04:09:44.490711Z","shell.execute_reply.started":"2025-04-27T04:09:43.296722Z","shell.execute_reply":"2025-04-27T04:09:44.489946Z"}},"outputs":[{"name":"stdout","text":"Connected to Qdrant at https://6afe4efa-992e-4445-af88-4f6a7ce7a4b1.europe-west3-0.gcp.cloud.qdrant.io\nCollection vietnamese_legal_parent already exists\nCollection vietnamese_legal_child already exists\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<qdrant_client.qdrant_client.QdrantClient at 0x7e1887bee610>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"!cd /\nwith open(\"/kaggle/input/samples2/law_documents_20250426_162027.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\nprint(f\"Loaded {len(data)} documents from samples.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T04:38:31.841548Z","iopub.execute_input":"2025-04-27T04:38:31.841816Z","iopub.status.idle":"2025-04-27T04:38:33.013961Z","shell.execute_reply.started":"2025-04-27T04:38:31.841790Z","shell.execute_reply":"2025-04-27T04:38:33.012979Z"}},"outputs":[{"name":"stdout","text":"Loaded 3952 documents from samples.json\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Extract text and document IDs\nlegal_texts = [item['noi_dung'] for item in data[:1000]]\ndocument_ids = [item['so_hieu'] for item in data[:1000]]\n\n# Prepare documents\ndocuments, metadata_store = prepare_vietnamese_legal_documents(legal_texts, document_ids)\n# After chunking, associate metadata with the chunks\n# for i, doc in enumerate(documents):\n#     doc.metadata = metadata_store[i]  # Link the metadata back to the chunk\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T04:55:44.807508Z","iopub.execute_input":"2025-04-27T04:55:44.807993Z","iopub.status.idle":"2025-04-27T04:55:47.506914Z","shell.execute_reply.started":"2025-04-27T04:55:44.807967Z","shell.execute_reply":"2025-04-27T04:55:47.506009Z"}},"outputs":[{"name":"stdout","text":"Preparing 1000 Vietnamese legal documents\n","output_type":"stream"},{"name":"stderr","text":"Processing documents: 100%|██████████| 1000/1000 [00:02<00:00, 372.32it/s]","output_type":"stream"},{"name":"stdout","text":"Created 3209 document objects\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"# Index to Qdrant\nnodes_info,all_nodes = index_documents_to_qdrant(documents)\n\nprint(\"Qdrant initialization complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T04:56:30.768599Z","iopub.execute_input":"2025-04-27T04:56:30.769184Z"}},"outputs":[{"name":"stdout","text":"Loaded bge-m3 embedding model\nParsing nodes hierarchically...\nCreated 5563 parent nodes and 12167 child nodes\nAdding parent nodes to Qdrant...\n","output_type":"stream"},{"name":"stderr","text":"Embedding parent nodes:   7%|▋         | 379/5563 [04:33<1:02:47,  1.38it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"all_nodes[0].metadata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:46:59.234488Z","iopub.execute_input":"2025-04-20T08:46:59.234737Z","iopub.status.idle":"2025-04-20T08:46:59.239704Z","shell.execute_reply.started":"2025-04-20T08:46:59.234721Z","shell.execute_reply":"2025-04-20T08:46:59.239010Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"{'document_id': '05/2025/TT-BYT',\n 'article': 'Điều 1 + Điều 2 + Điều 3',\n 'parent_links': ['Điều 1', 'Điều 2', 'Điều 3'],\n 'is_merged': True}"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"# test\nall_nodes = node_parser.get_nodes_from_documents(documents)\nprint(\"Sample node attributes:\", all_nodes[0].__dict__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:37:46.800567Z","iopub.execute_input":"2025-04-20T08:37:46.800860Z","iopub.status.idle":"2025-04-20T08:37:46.833444Z","shell.execute_reply.started":"2025-04-20T08:37:46.800840Z","shell.execute_reply":"2025-04-20T08:37:46.832493Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1728857543.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_nodes_from_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample node attributes:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'node_parser' is not defined"],"ename":"NameError","evalue":"name 'node_parser' is not defined","output_type":"error"}],"execution_count":25}]}